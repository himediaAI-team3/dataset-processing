# Generate_Output_Gemma.py - ë¡œì»¬ Gemmaë¡œ ì„¤ëª… ìƒì„± (GPU ë²„ì „)
# 
# [ì‚¬ìš© ë°©ë²•]
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜: pip install unsloth (GPU í™˜ê²½ í•„ìš”)
# 2. ê²½ë¡œ ì„¤ì •: DATASET_PATH, SAVE_PATH í™•ì¸
# 3. ì‹¤í–‰: python Generate_Output_Gemma.py
# 4. ê²°ê³¼: skin_disease_dataset_with_output í´ë” ìƒì„±
# 5. ì¥ì : ë¬´ë£Œ, ë¹ ë¦„ (RTX 3050 ê¸°ì¤€ 2-3ì‹œê°„), GPU ì‚¬ìš©
# 6. ì£¼ì˜: ì´ë¯¸ Claudeë¡œ ì²˜ë¦¬í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µí•˜ê³  ë‚˜ë¨¸ì§€ë§Œ ì²˜ë¦¬í•¨


from datasets import load_from_disk, DatasetDict
from unsloth import FastVisionModel
import torch
from tqdm import tqdm
import os
from prompts import SYSTEM_PROMPT

# ========== ì„¤ì • (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”!) ==========
DATASET_PATH = "./skin_disease_dataset"  # ì „ì²˜ë¦¬ (1)ì—ì„œ ë§Œë“  ë°ì´í„°ì…‹ ê²½ë¡œ
SAVE_PATH = "./skin_disease_dataset_with_output"  # ì €ì¥í•  ê²½ë¡œ
# ================================================

# System PromptëŠ” prompts.pyì—ì„œ import (INSTRUCTIONë¡œ ì‚¬ìš©)
INSTRUCTION = SYSTEM_PROMPT


def process_with_gemma(model, tokenizer, image, label, description, symptom):
    """
    Gemma ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
    
    Args:
        model: Gemma ëª¨ë¸
        tokenizer: Tokenizer
        image: PIL Image
        label: ì •ë‹µ ë¼ë²¨
        description: ì§ˆë³‘ ì„¤ëª…
        symptom: ì¦ìƒ
    
    Returns:
        ìƒì„±ëœ ì„¤ëª… (str)
    """
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = f"""ì •ë‹µì€ {label}ì´ë‹¤.

ì§ˆë³‘ íŠ¹ì§•: {description}
ì¦ìƒ: {symptom}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³ , ì •ë‹µì— ë§ê²Œ ì„¤ëª…í•˜ë¼."""
    
    # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": INSTRUCTION + "\n\n" + user_prompt},
                {"type": "image", "image": image}
            ]
        }
    ]
    
    # í† í°í™”
    inputs = tokenizer.apply_chat_template(
        conversation,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to("cuda")
    
    # ìƒì„±
    with torch.inference_mode():
        outputs = model.generate(
            inputs,
            max_new_tokens=512,
            temperature=0.3,
            do_sample=True,
            top_p=0.9
        )
    
    # ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # assistant ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    if "<|assistant|>" in response:
        response = response.split("<|assistant|>")[-1].strip()
    
    return response


def main():
    print("=" * 60)
    print("ì „ì²˜ë¦¬ (2) - ë¡œì»¬ Gemmaë¡œ ì„¤ëª… ìƒì„±")
    print("=" * 60)
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œëŠ” ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    print(f"\nğŸ“‚ Dataset ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {DATASET_PATH}")
    dataset = load_from_disk(DATASET_PATH)
    print(f"âœ… ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!")
    print(dataset)
    
    # Gemma ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    print(f"\nğŸ¤– Gemma ëª¨ë¸ ë¡œë”© ì¤‘...")
    model, tokenizer = FastVisionModel.from_pretrained(
        "unsloth/gemma-3-4b-it-unsloth-bnb-4bit",
        load_in_4bit=True,
        use_gradient_checkpointing="unsloth"
    )
    FastVisionModel.for_inference(model)  # ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    # Train ë°ì´í„° ì²˜ë¦¬
    print(f"\n" + "=" * 60)
    print("ğŸš€ Train ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    print("=" * 60)
    
    train_outputs = dataset["train"]["output"][:]
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ê°œìˆ˜ í™•ì¸
    already_done = sum(1 for x in train_outputs if x)
    print(f"ì´ë¯¸ ì²˜ë¦¬ëœ ë°ì´í„°: {already_done}ê°œ")
    print(f"ì²˜ë¦¬í•  ë°ì´í„°: {len(train_outputs) - already_done}ê°œ")
    
    # ì§„í–‰ ë£¨í”„
    for i in tqdm(range(len(dataset["train"])), desc="Processing Train"):
        # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ìŠ¤í‚µ
        if train_outputs[i]:
            continue
        
        try:
            sample = dataset["train"][i]
            
            # Gemmaë¡œ ìƒì„±
            result = process_with_gemma(
                model=model,
                tokenizer=tokenizer,
                image=sample["image"],
                label=sample["label"],
                description=sample["description"],
                symptom=sample["symptom"]
            )
            
            train_outputs[i] = result
            
        except Exception as e:
            print(f"\nâš ï¸ ERROR at index {i}: {str(e)}")
            train_outputs[i] = ""
        
        # 100ê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
        if (i + 1) % 100 == 0:
            print(f"\nğŸ’¾ ì¤‘ê°„ ì €ì¥ ì¤‘... ({i + 1}/{len(dataset['train'])})")
            train_dataset_updated = dataset["train"].remove_columns("output")
            train_dataset_updated = train_dataset_updated.add_column("output", train_outputs)
            
            temp_dataset = DatasetDict({
                "train": train_dataset_updated,
                "test": dataset["test"]
            })
            temp_dataset.save_to_disk(SAVE_PATH + "_temp")
            print("âœ… ì¤‘ê°„ ì €ì¥ ì™„ë£Œ!")
            
            # VRAM ì •ë¦¬
            torch.cuda.empty_cache()
    
    # ìµœì¢… Train output ì—…ë°ì´íŠ¸
    print(f"\nğŸ“Š Train ìµœì¢… ì—…ë°ì´íŠ¸ ì¤‘...")
    train_dataset_final = dataset["train"].remove_columns("output")
    train_dataset_final = train_dataset_final.add_column("output", train_outputs)
    
    # Test ë°ì´í„° ì²˜ë¦¬
    print(f"\n" + "=" * 60)
    print("ğŸš€ Test ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
    print("=" * 60)
    
    test_outputs = dataset["test"]["output"][:]
    
    for i in tqdm(range(len(dataset["test"])), desc="Processing Test"):
        if test_outputs[i]:
            continue
        
        try:
            sample = dataset["test"][i]
            
            result = process_with_gemma(
                model=model,
                tokenizer=tokenizer,
                image=sample["image"],
                label=sample["label"],
                description=sample["description"],
                symptom=sample["symptom"]
            )
            
            test_outputs[i] = result
            
        except Exception as e:
            print(f"\nâš ï¸ ERROR at index {i}: {str(e)}")
            test_outputs[i] = ""
        
        # VRAM ê´€ë¦¬
        if (i + 1) % 50 == 0:
            torch.cuda.empty_cache()
    
    # Test output ì—…ë°ì´íŠ¸
    test_dataset_final = dataset["test"].remove_columns("output")
    test_dataset_final = test_dataset_final.add_column("output", test_outputs)
    
    # ìµœì¢… Dataset ìƒì„±
    final_dataset = DatasetDict({
        "train": train_dataset_final,
        "test": test_dataset_final
    })
    
    # ì €ì¥
    print(f"\n" + "=" * 60)
    print(f"ğŸ’¾ ìµœì¢… Dataset ì €ì¥ ì¤‘: {SAVE_PATH}")
    print("=" * 60)
    os.makedirs(SAVE_PATH, exist_ok=True)
    final_dataset.save_to_disk(SAVE_PATH)
    
    print(f"\n" + "=" * 60)
    print("âœ… ì „ì²˜ë¦¬ (2) ì™„ë£Œ!")
    print("=" * 60)
    print(final_dataset)
    
    # ìƒ˜í”Œ í™•ì¸
    print(f"\nğŸ” ìƒ˜í”Œ í™•ì¸:")
    print(f"Label: {final_dataset['train'][0]['label']}")
    print(f"Output: {final_dataset['train'][0]['output'][:300]}...")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    del tokenizer
    torch.cuda.empty_cache()
    print(f"\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ!")


if __name__ == "__main__":
    main()